{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1a5fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import join\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.transforms import Compose , RandomCrop , ToTensor , ToPILImage ,CenterCrop ,Resize\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def is_image_file (filename):\n",
    "    return any(filename.endswith(extension) for extension in ['.png' , '.jpg' , '.jpeg'])\n",
    "\n",
    "def calculate_valid_crop_size(crop_size , upscale_factor):\n",
    "    return crop_size - (crop_size % upscale_factor)\n",
    "\n",
    "def train_hr_transform(crop_size):\n",
    "    return Compose([\n",
    "        RandomCrop(crop_size),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "\n",
    "def train_lr_transform(crop_size , upscale_factor):\n",
    "    return Compose([\n",
    "        ToPILImage(),\n",
    "        Resize(crop_size // upscale_factor , interpolation=Image.BICUBIC),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "def display_transform():\n",
    "    return Compose([\n",
    "        ToPILImage(),\n",
    "        Resize(400),\n",
    "        CenterCrop(400),\n",
    "        ToTensor()\n",
    "    ])\n",
    "\n",
    "class TrainDatasetFromFolder(Dataset):\n",
    "    def __init__(self , dataset_dir , crop_size , upscale_factor):\n",
    "        super(TrainDatasetFromFolder , self).__init__()\n",
    "        self.image_filenames = [join(dataset_dir , x) for x in listdir(dataset_dir) if is_image_file(x)]\n",
    "        crop_size = calculate_valid_crop_size(crop_size , upscale_factor)\n",
    "        self.hr_transform = train_hr_transform(crop_size)\n",
    "        self.lr_transform = train_lr_transform(crop_size , upscale_factor)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hr_image = self.hr_transform(Image.open(self.image_filenames[index]))\n",
    "        lr_image = self.lr_transform(hr_image)\n",
    "        return lr_image , hr_image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "class ValDatasetFromFolder(Dataset):\n",
    "    def __init__(self, dataset_dir, upscale_factor):\n",
    "        super(ValDatasetFromFolder, self).__init__()\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.image_filenames = [join(dataset_dir, x) for x in listdir(dataset_dir) if is_image_file(x)]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        hr_image = Image.open(self.image_filenames[index])\n",
    "        w, h = hr_image.size\n",
    "        crop_size = calculate_valid_crop_size(min(w, h), self.upscale_factor)\n",
    "        lr_scale = Resize(crop_size // self.upscale_factor, interpolation=Image.BICUBIC)\n",
    "        hr_scale = Resize(crop_size, interpolation=Image.BICUBIC)\n",
    "        hr_image = CenterCrop(crop_size)(hr_image)\n",
    "        lr_image = lr_scale(hr_image)\n",
    "        hr_restore_img = hr_scale(lr_image)\n",
    "        return ToTensor()(lr_image), ToTensor()(hr_restore_img), ToTensor()(hr_image)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e427f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.prelu = nn.PReLU()\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            channels, channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.conv1(x)\n",
    "        residual = self.bn1(residual)\n",
    "        residual = self.prelu(residual)\n",
    "        residual = self.conv2(residual)\n",
    "        residual = self.bn2(residual)\n",
    "\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class UpsampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, up_scale):\n",
    "        super(UpsampleBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels *\n",
    "                              (up_scale ** 2), kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(up_scale)\n",
    "        self.prelu = nn.PReLU()\n",
    "\n",
    "    def forward(self,  x):\n",
    "        x = self.conv(x)\n",
    "        x = self.pixel_shuffle(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, scale_factor) -> None:\n",
    "        upsample_block_num = int(math.log(scale_factor, 2))\n",
    "        super(Generator, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n",
    "            nn.PReLU()\n",
    "        )\n",
    "\n",
    "        self.block2 = ResidualBlock(64)\n",
    "        self.block3 = ResidualBlock(64)\n",
    "        self.block4 = ResidualBlock(64)\n",
    "        self.block5 = ResidualBlock(64)\n",
    "        self.block6 = ResidualBlock(64)\n",
    "        self.block7 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "        block8 = [UpsampleBlock(64, 2) for _ in range(upsample_block_num)]\n",
    "        block8.append(nn.Conv2d(64, 3, kernel_size=9, padding=4))\n",
    "        self.block8 = nn.Sequential(*block8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        block1 = self.block1(x)\n",
    "        block2 = self.block2(block1)\n",
    "        block3 = self.block3(block2)\n",
    "        block4 = self.block4(block3)\n",
    "        block5 = self.block5(block4)\n",
    "        block6 = self.block6(block5)\n",
    "        block7 = self.block7(block6)\n",
    "        block8 = self.block8(block1 + block7)\n",
    "\n",
    "        return (torch.tanh(block8) + 1) / 2\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 1024, kernel_size=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(1024, 1, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = self.net(x)\n",
    "        x = x.view(batch_size, -1)  # Reshape x without modifying it in-place\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b48eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models.vgg import vgg16\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "\n",
    "class GeneratorLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GeneratorLoss, self).__init__()\n",
    "        vgg = vgg16(pretrained=True)\n",
    "        loss_network = nn.Sequential(*list(vgg.features)[:31]).eval()\n",
    "        for param in loss_network.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.loss_network = loss_network\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.tv_loss = TVLoss()\n",
    "\n",
    "    def forward(self, out_labels, out_images, target_images):\n",
    "        # Adversarial Loss\n",
    "        # we want real_out to be close 1, and fake_out to be close 0\n",
    "        adversarial_loss = torch.mean(1 - out_labels)\n",
    "        # Perception Loss\n",
    "        perception_loss = self.mse_loss(self.loss_network(out_images), self.loss_network(target_images))\n",
    "        # Image Loss\n",
    "        image_loss = self.mse_loss(out_images, target_images)\n",
    "        # TV Loss \n",
    "        tv_loss = self.tv_loss(out_images)\n",
    "        return image_loss + 0.001 * adversarial_loss + 0.006 * perception_loss + 2e-8 * tv_loss\n",
    "\n",
    "\n",
    "class TVLoss(nn.Module):\n",
    "    def __init__(self, tv_loss_weight=1):\n",
    "        super(TVLoss, self).__init__()\n",
    "        self.tv_loss_weight = tv_loss_weight\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.tv_loss_weight * 0.5 * (\n",
    "            torch.abs(x[:, :, 1:, :] - x[:, :, :-1, :]).mean() + \n",
    "            torch.abs(x[:, :, :, 1:] - x[:, :, :, :-1]).mean()\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef0e8bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x1a7ec87d780>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import time \n",
    "from PIL import Image \n",
    "import os\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data \n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import pytorch_ssim\n",
    "from data_utils import TrainDatasetFromFolder , ValDatasetFromFolder , display_transform\n",
    "from srgan_loss import GeneratorLoss\n",
    "from srgan import Generator , Discriminator\n",
    "from math import log10\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e3b0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--num_epochs'], dest='num_epochs', nargs=None, const=None, default=100, type=<class 'int'>, choices=None, required=False, help='Train epoch number', metavar=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='Train Super Resolution Models')\n",
    "parser.add_argument('--crop_size' , default=88 , type=int , help='training images crop size')\n",
    "parser.add_argument('--upsacale_factor' , default= 4 , type=int, choices=[2,4,8] , help=\"Super Resolution Upscale Factor\")\n",
    "parser.add_argument(\"--num_epochs\" , default=100 , type= int , help='Train epoch number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7ef4dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crop Size: 88\n",
      "Upscale Factor: 4\n",
      "Number of Epochs: 100\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "# Define a list of command-line arguments\n",
    "args = ['--crop_size', '88', '--upsacale_factor', '4', '--num_epochs', '100']\n",
    "\n",
    "# Create an ArgumentParser and parse the arguments\n",
    "parser = argparse.ArgumentParser(description='Train Super Resolution Models')\n",
    "parser.add_argument('--crop_size', default=88, type=int, help='training images crop size')\n",
    "parser.add_argument('--upsacale_factor', default=4, type=int, choices=[2, 4, 8], help=\"Super Resolution Upscale Factor\")\n",
    "parser.add_argument(\"--num_epochs\", default=100, type=int, help='Train epoch number')\n",
    "\n",
    "opt = parser.parse_args(args)\n",
    "\n",
    "# Now you can access the parsed arguments as opt.crop_size, opt.upsacale_factor, etc.\n",
    "print(\"Crop Size:\", opt.crop_size)\n",
    "print(\"Upscale Factor:\", opt.upsacale_factor)\n",
    "print(\"Number of Epochs:\", opt.num_epochs)\n",
    "\n",
    "\n",
    "\n",
    "CROP_SIZE = opt.crop_size\n",
    "UPSCALE_FACTOR = opt.upsacale_factor\n",
    "NUM_EPOCHS = opt.num_epochs\n",
    "\n",
    "train_set = TrainDatasetFromFolder('datasets/div2k/train' , crop_size=CROP_SIZE , upscale_factor=UPSCALE_FACTOR)\n",
    "val_set = ValDatasetFromFolder('datasets/div2k/valid' , upscale_factor=UPSCALE_FACTOR)\n",
    "train_loader = DataLoader(dataset=train_set , batch_size=64 , shuffle=True , num_workers=4)\n",
    "val_loader = DataLoader(dataset=val_set , num_workers=4 , batch_size=1 , shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e16014b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# generator parameters: 733579\n",
      "# discriminator parameters: 5215425\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    netG = Generator(UPSCALE_FACTOR)\n",
    "    print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n",
    "    netD = Discriminator()\n",
    "    print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc19647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Piyush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Piyush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "generator_criterion = GeneratorLoss()\n",
    "if torch.cuda.is_available():\n",
    "        netG.cuda()\n",
    "        netD.cuda()\n",
    "        generator_criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8356e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG = optim.Adam(netG.parameters())    \n",
    "optimizerD = optim.Adam(netD.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbed5ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    " results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [] , 'ssim': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4fc44d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1/100] Loss_D: 0.5155 Loss_G: 0.0450 D(x): 0.6553 D(G(z)): 0.1708: 100%|██████████████| 13/13 [03:24<00:00, 15.76s/it]\n",
      "[converting LR images to SR images] PSNR: 12.6945 dB SSIM: 0.3827: 100%|█████████████| 100/100 [29:16<00:00, 17.57s/it]\n",
      "[saving training results]: 100%|███████████████████████████████████████████████████████| 20/20 [00:30<00:00,  1.54s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory epochs does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 95\u001b[0m\n\u001b[0;32m     92\u001b[0m         index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# save model parameters\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepochs/netG_epoch_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mUPSCALE_FACTOR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(netD\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs/netD_epoch_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (UPSCALE_FACTOR, epoch))\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# save loss\\scores\\psnr\\ssim\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[1;34m(name_or_buffer)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[1;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Parent directory epochs does not exist."
     ]
    }
   ],
   "source": [
    "for epoch in range( 1 , NUM_EPOCHS + 1):\n",
    "        train_bar = tqdm(train_loader)\n",
    "        running_results = {'batch_sizes': 0 , 'd_loss': 0 , 'g_loss': 0 , 'd_score': 0 , 'g_score': 0}\n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        for data , target in train_bar:\n",
    "            g_update_first = True\n",
    "            batch_size = data.size(0)\n",
    "            running_results['batch_sizes'] += batch_size\n",
    "            # Update D network\n",
    "            # We want our real to be close to 1 and fake to be close to 0\n",
    "\n",
    "            real_img = Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                real_img = real_img.cuda()\n",
    "            z = Variable(data)\n",
    "            if torch.cuda.is_available():\n",
    "                z = z.cuda()\n",
    "            fake_img = netG(z)\n",
    "\n",
    "           # Update D network\n",
    "            netD.zero_grad()\n",
    "            real_out = netD(real_img).mean()\n",
    "            fake_out = netD(fake_img).mean()\n",
    "            d_loss = 1 - real_out + fake_out\n",
    "            d_loss.backward(retain_graph=True)  # Specify retain_graph=True\n",
    "\n",
    "            # Update G network \n",
    "            netG.zero_grad()\n",
    "            g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "            g_loss.backward()\n",
    "\n",
    "            optimizerG.step()\n",
    "            optimizerD.step()  # You can move this line here to update the discriminator after computing both losses\n",
    "\n",
    "            # loss for current batch before optimization\n",
    "            running_results['g_loss'] += g_loss.item() * batch_size\n",
    "            running_results['d_loss'] += d_loss.item() * batch_size\n",
    "            running_results['d_score'] += real_out.item() * batch_size\n",
    "            running_results['g_score'] += fake_out.item() * batch_size\n",
    "\n",
    "\n",
    "            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'], \n",
    "                running_results['g_loss'] / running_results['batch_sizes'], \n",
    "                running_results['d_score'] / running_results['batch_sizes'], \n",
    "                running_results['g_score'] / running_results['batch_sizes']\n",
    "            ))\n",
    "\n",
    "        netG.eval()\n",
    "        out_path = 'training_results/SRF_' + str(UPSCALE_FACTOR) + '/'\n",
    "        if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(val_loader)\n",
    "            valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "            val_images = []\n",
    "            for val_lr, val_hr_restore, val_hr in val_bar:\n",
    "                batch_size = val_lr.size(0)\n",
    "                valing_results['batch_sizes'] += batch_size\n",
    "                lr = val_lr\n",
    "                hr = val_hr\n",
    "                if torch.cuda.is_available():\n",
    "                    lr = lr.cuda()\n",
    "                    hr = hr.cuda()\n",
    "                sr = netG(lr)\n",
    "\n",
    "                batch_mse = ((sr - hr) ** 2).data.mean()\n",
    "                valing_results['mse'] += batch_mse * batch_size\n",
    "                batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
    "                valing_results['ssims'] += batch_ssim * batch_size\n",
    "                valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "                valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
    "                val_bar.set_description(\n",
    "                    desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
    "                        valing_results['psnr'], valing_results['ssim']\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                val_images.extend(\n",
    "                    [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)), \n",
    "                     display_transform()(sr.data.cpu().squeeze(0))]\n",
    "                )\n",
    "            val_images = torch.stack(val_images)\n",
    "            val_images = torch.chunk(val_images, val_images.size(0) // 15)\n",
    "            val_save_bar = tqdm(val_images, desc='[saving training results]')\n",
    "            index = 1\n",
    "            for image in val_save_bar:\n",
    "                image = utils.make_grid(image, nrow=3, padding=5)\n",
    "                utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n",
    "                index += 1\n",
    "        \n",
    "        # save model parameters\n",
    "        torch.save(netG.state_dict(), 'epochs/netG_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "        torch.save(netD.state_dict(), 'epochs/netD_epoch_%d_%d.pth' % (UPSCALE_FACTOR, epoch))\n",
    "        # save loss\\scores\\psnr\\ssim\n",
    "        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
    "        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
    "        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
    "        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
    "        results['psnr'].append(valing_results['psnr'])\n",
    "        results['ssim'].append(valing_results['ssim'])\n",
    "\n",
    "        if epoch % 10 == 0 and epoch != 0:\n",
    "            out_path = \"statistics/\"\n",
    "            data_frame = pd.DataFrame(\n",
    "                data = {'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'], \n",
    "                        'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim'], }, \n",
    "                        index=range(1, epoch+1)\n",
    "            )\n",
    "            data_frame.to_csv(out_path + 'srf_' + str(UPSCALE_FACTOR) + '_train_results.csv', index_label='Epoch')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ed8a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fdbbb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f174d777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446722db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b776f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8904c66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
